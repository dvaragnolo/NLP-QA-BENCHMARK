# NLP-QA-BENCHMARK
This page contains the evaluation results for Question Answering system presented in _"An Ontology-Based Question-Answering, from Natural Language to SPARQL Query"_ Varagnolo D., Melo D., Rodrigues I. P. (2023).

The architecture of the system includes a pipeline with three modules: 
* Partial Semantic Representation, composed by two steps : the dependency parser of Stanza, and transformation into a set of partial Discourse Representation Structures (DRSs).
* Pragmatic interpretation, which rewrites the partial semantic representations of a question into a set of variant meanings of the question in the domain-specific context.
* SPARQL Generator, which is applied to the Semantic Query Representation solution and generates the corresponding SPARQL query representation.

The repository is structured as follows: Each set contains a directory, within which there is a collection of input questions for the system (in the “questions” directory), and a directory with the results for each question in the set. The results are presented in a comma-separated values (CSV) format for each module of the system pipeline, in the “results” directory.
In addition, the results file includes a comparison between the SPARQL response generated by the query (“Gen. Query Response”) and a manually validated SPARQL response retrieved from a manually created SPARQL query (“Man. Query Response”). The latter can be found in the “validation” directory.
The comparison is conducted by checking if the Gen. Query Response contains the Man. Query Response. More details on this comparison can be found in the algorithm description file.


## Evaluation Set
The main evaluation set is made by 43 different questions on Smithsonian American Art Museum (SAAM) Knowledge Base. The collection of natural language questions include several different request on art domain, asking information about artists and artefact. 

## CIDOC-QA-BENCHMARK Set

The second evaluation is about a set of 5'000 questions based on 10 query templates (of different radius). These questions are retrieved from 
N. GOUNAKIS, M. MOUNTANTONAKIS, and Y. TZITZIKAS "Evaluating a Radius-based Pipeline for Question Answering over Cultural
(CIDOC-CRM based) Knowledge Graphs" (2023). The datasets can be found at this link https://github.com/NicolaiGoon/CIDOC-QA-BENCHMARK/. The directory CIDOC-QA-BENCHMARK includes the results grouped by template, with 500 samples for each one. 

### Templates
In the CIDOC-QA-BENCHMARK repository, can be found the SPARQL template queries that sent to <https://triplydb.com/smithsonian/american-art-museum/sparql/american-art-museum>. The placeholders {Art Work} and {Artist} are replaced with entities present in the KG. From original templates, we add the following filtrer on SPARQL Query to specify the label of Art Work or Artist).

Case {Art Work}

```sparql
[...]
Filter(REGEX(str(?label), "{Art Work}","i")).
[...]
```

Case {Artist}

```sparql
[...]
Filter(REGEX(str(?label), "{Artist}","i")).
[...]
```
